\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1,T2A]{fontenc}
\usepackage{graphics,graphicx}
\usepackage[usenames]{color}
\usepackage{colortbl}
\usepackage{a4wide}
\usepackage{amssymb,amsfonts,amsthm,amsmath,mathtext,cite,enumerate,float}
\usepackage[english,russian]{babel}
\usepackage[all]{xy}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage[colorlinks,urlcolor=blue]{hyperref}
\parindent=2cm

\begin{document}



%define code style
\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=C++,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  commentstyle=\itshape\color{purple!40!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
}
%

\section{Решающие деревья}
\subsection{Рекурсивное построение дерева (LearnID3)}
Пусть $U$ - обучающая выборка, $B$ - множество предикатов. На каждом шаге будет максимально точно разделять элементы по клвссам.
\begin{enumerate}
\item \textbf{if} Все элементы лежат в одном классе $c\subset Y$\\\
создать новый лист $\upsilon$\\\
$c_\upsilon:=c$\\
\textbf{return} $\upsilon$
\item Найти предикат с максимальной информативностью $\beta:=argmaxI(\beta,U)$
\item Разбить выборку по предикату $\beta$ на две части

$U=U_0 \cup U_1$: \\\
$U_0 = \lbrace x \subset U:\beta(x) = 0 \rbrace $\\\
$U_1 = \lbrace x \subset U:\beta(x) = 1 \rbrace $
\item \textbf{if} $U_0 = \emptyset$ или $U_1 = \emptyset$\\\
	создать новый лист $\upsilon$\\\
	$c_\upsilon:=$класс, в котором находится большинство 			элементов из $U$\\
\textbf{else}\\\
	создать новую внутреннюю вершину $\upsilon$\\\
	$\beta_\upsilon:=\beta$\\\
	$L_\upsilon:=LearnID3(U_0)$\\\
	$R_\upsilon:=LearnID3(U_1)$
\item \textbf{return} $\upsilon$
\end{enumerate}
\textbf{Критерий Гини} - мера того, как часто случайный элемент выборки будет неправильно отнесен к тому или иному классу: 

$$H_G = \Sigma p_i(1-p_i) = \Sigma (p_i) - \Sigma (p^2_i) = 1 - \Sigma (p^2_i) $$
\textbf{Энтропийный критерий}: - мера того, как распределение классов отличается от вырожденного\\
Определим энтропию как $$H_E = -\Sigma p_i lnp_i$$
для двух классов: $$H_E = -plnp -(1-p)ln(1-p)$$
Для регрессии - среднеквадратичная ошибка:
$$H_R= \frac{1}{|X|}\Sigma(y_i - y_{mean})^2$$
\subsection{Многоклассовые решающие деревья}
См. предыдущий пункт.
\subsection{Модификация регрессионного дерева}
Не совсем понятно условие.
\subsection{Решающее правило в листе}
Думаю случайно, иначе ошибка может начать быстро накапливаться.
\subsection{Unsupervised decision tree}
Для непрерывного распределения:
$$H= -\int^{\infty}_{-\infty}f(x)lnf(x)dx$$
Для удобства пусть $ \mu = \textbf{0}$ (осторожно, много сигм)\\\
$$H = \frac{1}{2}E[\Sigma_{i,j}X_i(\Sigma)^{-1}_{ij}X_i]+
\frac{1}{2}ln(2\pi)^n|\Sigma|$$
Первое слагаемое:
$$ \frac{1}{2}E[\Sigma_{i,j}X_i(\Sigma)^{-1}_{ij}X_i]=
 \frac{1}{2}E[\Sigma_{i,j}X_iX_i(\Sigma)^{-1}_{ij}]=
 \frac{1}{2}\Sigma_{i}\Sigma_{j}E[X_iX_i(\Sigma)^{-1}_{ij}]=
 \frac{1}{2}\Sigma_{i}\Sigma_{j}(\Sigma)_{ij}(\Sigma)^{-1}_{ij}]=
 \frac{1}{2}trI_n$$
Таким образом:
$$H=\frac{1}{2}n + \frac{1}{2}ln(2\pi)^n|\Sigma| = \frac{1}{2}ln(2\pi e)^n|\Sigma|$$
\section{Линейные классификаторы}
\subsection{Знакомство с линейным классификатором}
\begin{enumerate}
\item Пусть объекты описываются числовыми признаками $f_i:X\rightarrow R, i=1,...,n$, $Y$ - множество имен классов, тогда
$a:X\rightarrow Y$ называется линейным классификатором множества объектов во множество признаков.
$$a(X,\omega):=argmax_{y\in Y} sign(\Sigma \omega_j f_j(x)- \omega_0),$$
где $\omega_j$ - вес $j$-го признака, $\omega_0$ - порог принятия решения
\item Пусть, как и раньше $Y=\lbrace -1,1 \rbrace$, для $x_i\in X^m$ отступом алгоритма на объекте называется 
$$M(x_i)=y_i<x_i,\omega >$$
Чем меньше отступ, тем ближе объект к границе классов, и тем выше вероятность ошибки. $M(x_i)<0\Leftrightarrow a(x)$ допускает ошибку на объекте $x_i$.
\newpage
\item Введем фиктивный константный признак $f_0 = -1$, тогда
$$-\omega_0 + <\omega,x> = x_0\omega_0 + <\omega,x> = <\omega,x'>$$
\item $Q(\omega)=\Sigma[a(x_i,\omega)\neq y_i]=\Sigma[M(x_i)<0]\rightarrow min$
\item Вопрос не совсем понятен. Можно выбрать вектор весов $\omega=argminQ$, либо так, чтобы $-signy_i = sign<x_i,\omega>$.
\item $Q = \Sigma L(y_i,f(x_i))$
\item Пусть мы хотим знать точный ответ(если мы вообще можем его знать), но довольно близкий к точному. Ограничим $[M<0]$ сверху функцией $L(M)$  которая может быть например непрерывной или даже гладкой, что позволит нам использовать большее количество различных методов оптимизации, например метод градиентного спуска, тогда
$$Q(\omega)\leq Q'(\omega) = \Sigma L(M(x_i)).$$
Обычно функция потерь стремится к нулю на $+\infty$ и возрастает на $-\infty$ (правильная коассификация, устойчивость к выбросам, уменьшение переобучения).
\item Практический смысл квадратичной функции потерь - метод наименьших квадратов(нпр для регрессии).\\\
Можно выбрать симметричную функцию потерь(квадратичную как выше), тогда квадрат отклонения прогноза от фактического значения имеет одинаковую оошибку. Математическое ожидание ошибки прогнозирования равно нулю.
\item $L = |M|$\\\
$ L = 0$ if $|M|< \frac{\epsilon}{2}$  $ L= \frac{1}{\epsilon}$ if $|M| \geq \frac{\epsilon}{2} $
\item Добавим слогаемое(регуляризатор) $\gamma V(\omega)$ в функционал эмпирического риска
$$ Q(\omega) = \Sigma L(M(x_i)) + \gamma V(\omega)$$
и будем минимизировать полученное выражение. Регуляризатор снижает риск переобучения и повышает устойчивость вектора весов к шумам обучающей выборки. В качестве функционала регуляризатора можно взять норму вектора весов с некоторым коеффициентом. Переобучение может проявляться например в сильном увеличении вектора весов, тогда будем выбирать $\gamma < 1$.
\begin{itemize}
\item $p=2$: $V_2(\omega)=\gamma\Sigma\omega^2_k$ - Гауссовский
\item $p=1$: $V_1(\omega)=\gamma\Sigma |\omega_k|$ - Лапласовский
\item $V_e = \alpha V_1 + \beta V_2$ - elastic net
\end{itemize}
\item Переобучение - следствие плохой обобщающей способности. Можно придумать алгоритм, который будет иметь плохую обобщающую способность вне зависимости от наличия регуляризатора.
\item Например в градиентных методах, шаг для $\omega$ зависит от производной $L(M)$. Острые минимумы значат неограниченность производной, а значит могут повлечь за собой переобучение.
\item Органичивает веса признаков.
\item Функционал аппроксимированного риска будет принимать большее значение с регуляризатором. Без регуояризатора он может всюду быть равен нулю, однако равенство нулю не гарантирует хорошую обобщающую способность.
\item На тестовой выборке - наоборот, большее значение будет без регуляризации, т.к веса могут сильно возрастать вследствие переобучения.
\end{enumerate}
\subsection{Вероятностный смысл регуляризаторов}
$$-Q = -\Sigma L(y_i,<\omega ,x_i>) - \gamma ||\omega||^2 = -lnp_i -lnq = -lnq\prod p_i \rightarrow max$$
логарифм - монотонная функция
$$-q\prod p_i \rightarrow max$$
по принципу максимального правдоподобия
$$p(x,\mu, \Sigma)= \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}
e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$$
что есть Гауссовское распределение. Пусть $\mu = \textbf{0}, \Sigma = I$, тогда
$$p= \frac{1}{(2\pi)^{\frac{n}{2}}}e^{-\frac{1}{2}||x||^2}$$
Для $l_1$ аналогично получается распределение Лапласа:
$$p=Ce^{-\alpha |x|}$$
\section{Метрики качества бинарной классификации}
\begin{enumerate}
\item
\begin{itemize}
\item accuracy = $\frac{TP + TN}{TP + TN + FN + FP}$ - доля правильных ответов
\item precision = $\frac{TP}{TP + FP}$ - точность. Показывает какая доля выделенных классификатором объектов как положительной, действительно являются положительными.
\item recall = $\frac{TP}{TP + FN}$ - полнота. Показывает, какая часть положительных объектов была выделена классификатором.
\end{itemize}
\item ROC-кривая - график, позволяющий оценить качество бинарной классификации,  отображает соотношение между долей объектов, верно классифицированных, как TP, и долей объектов несущих признак FP. AUC - количественная интерпретация ROC-кривой - площадь под ROC-кривой. Чем выше AUC, тем качественней классификатор. Худшая кривая пройдет по диагонали $(0,0)(1,1)$, тогда площадь будет равна $0.5$.
\item Алгоритм построения ROC-кривой:
\begin{enumerate}
\item[1.] $l_{-}$- число объектров класса -1\\\
		  $l_{+}$- число объектров класса +1
\item[2.] Упорядочить выборку $X^l$ по убыванию $f(x_i)=<\omega, x_i>$
\item[3.] Поставить первую точку в начало координат:\\\
		  $(FPR_0,TPR_0):= (0,0), AUC:= 0$
\item[4.] \textbf{for} $i=1,...,l$\\\
			$\textbf{if} y_i = -1$\\\
			сместиться на шиг вправо:\\\
			$FPR_i:=FPR_{i-1}+\frac{1}{l_{-}}$\\\
			$TPR_i:=TPR_{i-1}$\\\
			$AUC:=AUC+\frac{1}{l_{-}}TPR_i$\\\
		 \textbf{else} \\\
		 сместиться на шаг вверх:\\\
		 $FPR_i:=FPR_{i-1}$\\\
		 $TPR_i:=TPR_{i-1}+\frac{1}{l_{+}}$\\\
		
\end{enumerate}
\item AUC-ROC предпочтительней, когда классы объектов довольно сильно различаются по количеству.
\item

\end{enumerate}
\section{Реализация решающего дерева}
TODO(конфликт с модулями, допишу потом)
\section{Исследование свойств решающих деревьев}
Ссыль в письме

\section{Дополнительные сведения о решающих деревьях}
\begin{itemize}
\item \textbf{C4.5} в отличие от \textbf{ID3} использует нормализованный прирост информации
\begin{enumerate}
\item Для каждого атрибута $a$ найти нормализованный прирост информации от разбиения
\item Пусть $a_0$ - атрибут с наибольшим нормализованным приростом информации
\item Создать вершину от разбиения по $a_0$
\item Вызвать алгоритм рекурсивно на дочерних вершинах\\\
\end{enumerate}
Прунинг - после создания дерева делается проход к корню, и наиболее значимые вершины заменяются на листья.
\item \textbf{CART}\\\
Идея прунинга: первым делом отсекать те поддеревья, которые, в зависимости от их размера, дадут меньший вклад в ошибку на тестовых данных. Увеличение ошибки оценивается некоторой величиной $\alpha$, которая есть средняя ошибка на каждый лист поддерева. CART использует $\alpha$ для генерации последовательность(уменьшающуюся) меньших деревьев: накаждой итерации от отсекает все поддеревья, дающих меньшее значение $\alpha$.
\end{itemize}
В \textbf{С5} так же используется кросс валидация, критерий Гини и др.

\section{PS}
С днем рождения, надеюсь ты проверишь задание на мой др таки пришлешь информацию по диплому.\\\
Короч, мне лень искать картинку, так что вот тебе телка из плейбоя.\\\
\newpage

\includegraphics*{chick.jpeg}
\end{document}
